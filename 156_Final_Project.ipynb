{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 156 Project Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to convert as much LOB into 2d heatmaps as possible. The heatmap conversion needs a lot of work to ensure it can be properly fed into a convolutional NN. The output of the CNN should be a 1, 2, or 3 that reflects whether the mid-price movement is (1) downward, (2) stationary, (3) upward. We will evaluate the efficacy of out CNN through win-rate.\n",
    "\n",
    "The dataset is so large, so we will (probably) only use one of the datasets for training and one for testing. They are already split into normalized cross-fold training and testing data, so we will just use the NoAuction, min-max normalized, CF1 training and testing dataset. \n",
    "\n",
    "The last five rows of the dataset represents the price movement (1, 2, or 3), while the first 144 rows are the predictors. The first 40 rows are the only rows we will be using. They are in the following order:\n",
    "Rows 1-10: Bid prices with 1 being the best, 10 being the worst\n",
    "Rows 11-20: Ask prices with 11 being the best, 20 being the worst\n",
    "Rows 21-30: Bid volumes associated with each bid price\n",
    "Rows 31-40: Ask volumes associated with each ask price\n",
    "\n",
    "Here is the dataset link and the paper associated with it, respectively:\n",
    "https://etsin.fairdata.fi/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649\n",
    "https://arxiv.org/pdf/1705.03233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 4992.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[6.0150044e-01 6.0326567e-01 6.0238305e-01 6.0238305e-01]\n",
      "  [6.0150044e-01 6.0282436e-01 6.0238305e-01 6.0326567e-01]\n",
      "  [6.0105914e-01 6.0194175e-01 6.0238305e-01 6.0282436e-01]\n",
      "  [6.0105914e-01 6.0194175e-01 6.0238305e-01 6.0282436e-01]\n",
      "  [6.0105914e-01 6.0194175e-01 6.0238305e-01 6.0282436e-01]\n",
      "  [6.0105914e-01 6.0194175e-01 6.0238305e-01 6.0238305e-01]\n",
      "  [6.0105914e-01 6.0238305e-01 6.0238305e-01 6.0238305e-01]\n",
      "  [6.0238305e-01 6.0238305e-01 6.0194175e-01 6.0238305e-01]\n",
      "  [6.0238305e-01 6.0238305e-01 6.0194175e-01 6.0282436e-01]\n",
      "  [6.0238305e-01 6.0238305e-01 6.0194175e-01 6.0282436e-01]]\n",
      "\n",
      " [[1.4368520e-02 3.4655890e-02 3.3349661e-02 8.2455711e-03]\n",
      "  [8.5721283e-03 1.0327374e-02 3.3349661e-02 2.5308188e-03]\n",
      "  [4.9391787e-03 1.0164095e-02 3.3349661e-02 8.0006531e-03]\n",
      "  [1.3103110e-02 5.0616377e-03 3.3349661e-02 8.0006531e-03]\n",
      "  [1.3103110e-02 5.0616377e-03 3.3349661e-02 8.0006531e-03]\n",
      "  [1.3103110e-02 5.0616377e-03 3.3349661e-02 9.2660625e-03]\n",
      "  [4.4901625e-04 8.2455711e-03 3.3349661e-02 9.2660625e-03]\n",
      "  [3.7431627e-02 1.6409503e-02 8.2455711e-03 9.2660625e-03]\n",
      "  [3.3349661e-02 8.2455711e-03 8.2455711e-03 1.2205078e-02]\n",
      "  [3.3349661e-02 8.2455711e-03 8.2455711e-03 1.2205078e-02]]\n",
      "\n",
      " [[6.0142033e-01 6.0230803e-01 6.0408344e-01 6.0230803e-01]\n",
      "  [6.0142033e-01 6.0230803e-01 6.0408344e-01 6.0230803e-01]\n",
      "  [6.0142033e-01 6.0230803e-01 6.0408344e-01 6.0230803e-01]\n",
      "  [6.0142033e-01 6.0230803e-01 6.0408344e-01 6.0230803e-01]\n",
      "  [6.0142033e-01 6.0186418e-01 6.0408344e-01 6.0230803e-01]\n",
      "  [6.0186418e-01 6.0186418e-01 6.0408344e-01 6.0230803e-01]\n",
      "  [6.0186418e-01 6.0186418e-01 6.0319574e-01 6.0230803e-01]\n",
      "  [6.0363959e-01 6.0186418e-01 6.0142033e-01 6.0230803e-01]\n",
      "  [6.0363959e-01 6.0230803e-01 6.0230803e-01 6.0230803e-01]\n",
      "  [6.0408344e-01 6.0230803e-01 6.0230803e-01 6.0230803e-01]]\n",
      "\n",
      " [[5.4166667e-03 1.6500000e-03 8.3166667e-03 8.3166667e-03]\n",
      "  [5.4166667e-03 1.6500000e-03 8.3166667e-03 8.3166667e-03]\n",
      "  [5.4166667e-03 1.6500000e-03 8.3166667e-03 8.3166667e-03]\n",
      "  [5.4166667e-03 1.6500000e-03 8.3166667e-03 8.3166667e-03]\n",
      "  [5.4166667e-03 6.8666667e-03 8.3166667e-03 3.3166667e-03]\n",
      "  [3.4666667e-03 5.0833333e-03 8.3166667e-03 3.3166667e-03]\n",
      "  [6.8000000e-03 5.0833333e-03 1.6500000e-03 3.3166667e-03]\n",
      "  [6.6500000e-03 5.0833333e-03 5.4166667e-03 3.3166667e-03]\n",
      "  [6.6500000e-03 8.3166667e-03 1.6500000e-03 3.3166667e-03]\n",
      "  [8.3166667e-03 8.3166667e-03 1.6500000e-03 3.3166667e-03]]\n",
      "\n",
      " [[6.0238305e-01 6.0326567e-01 6.0238305e-01 6.0282436e-01]\n",
      "  [6.0282436e-01 6.0282436e-01 6.0238305e-01 6.0326567e-01]\n",
      "  [6.0105914e-01 6.0194175e-01 6.0282436e-01 6.0282436e-01]\n",
      "  [6.0194175e-01 6.0194175e-01 6.0282436e-01 6.0282436e-01]\n",
      "  [6.0194175e-01 6.0194175e-01 6.0282436e-01 6.0282436e-01]\n",
      "  [6.0194175e-01 6.0194175e-01 6.0282436e-01 6.0238305e-01]\n",
      "  [6.0150044e-01 6.0238305e-01 6.0282436e-01 6.0238305e-01]\n",
      "  [6.0282436e-01 6.0238305e-01 6.0194175e-01 6.0238305e-01]\n",
      "  [6.0238305e-01 6.0238305e-01 6.0194175e-01 6.0458959e-01]\n",
      "  [6.0238305e-01 6.0238305e-01 6.0194175e-01 6.0282436e-01]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def load_fi2010_data(file_path, num_levels=10):\n",
    "    \"\"\"\n",
    "    Load the FI-2010 dataset from a .txt file and extract the top 'num_levels' bid-ask levels.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the FI-2010 .txt dataset.\n",
    "    - num_levels (int): Number of bid-ask levels to extract.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with top bid-ask levels.\n",
    "    \"\"\"\n",
    "    col_names = []\n",
    "    \n",
    "    # Define column names for bid/ask prices and sizes\n",
    "    for i in range(1, num_levels + 1):\n",
    "        col_names.append(f\"ask_price_{i}\")\n",
    "        col_names.append(f\"ask_size_{i}\")\n",
    "        col_names.append(f\"bid_price_{i}\")\n",
    "        col_names.append(f\"bid_size_{i}\")\n",
    "\n",
    "    # Load dataset from .txt file (space-separated)\n",
    "    data = pd.read_csv(file_path, delimiter=r'\\s+', header=None)\n",
    "    \n",
    "    # Select the first (num_levels * 4) columns corresponding to LOB data\n",
    "    data = data.iloc[:, : num_levels * 4]\n",
    "    data.columns = col_names\n",
    "    \n",
    "    return data\n",
    "\n",
    "def convert_to_2d_matrix(bid_prices, ask_prices, bid_volumes, ask_volumes):\n",
    "    \"\"\"\n",
    "    Convert the bid/ask data into a 10x4 matrix for CNN input.\n",
    "    \n",
    "    Args:\n",
    "    - bid_prices (np.array): Normalized bid prices (10 levels).\n",
    "    - ask_prices (np.array): Normalized ask prices (10 levels).\n",
    "    - bid_volumes (np.array): Normalized bid volumes (10 levels).\n",
    "    - ask_volumes (np.array): Normalized ask volumes (10 levels).\n",
    "    \n",
    "    Returns:\n",
    "    - 10x4 matrix (height x width), where height = number of price levels (10) and width = 4.\n",
    "    \"\"\"\n",
    "    # Create a 10x4 matrix for each snapshot\n",
    "    lob_matrix = np.zeros((10, 4))  # 10 price levels and 4 features (Ask Price, Bid Price, Ask Volume, Bid Volume)\n",
    "    \n",
    "    # Assign data to the matrix\n",
    "    lob_matrix[:, 0] = ask_prices  # Ask Prices\n",
    "    lob_matrix[:, 1] = bid_prices  # Bid Prices\n",
    "    lob_matrix[:, 2] = ask_volumes  # Ask Volumes\n",
    "    lob_matrix[:, 3] = bid_volumes  # Bid Volumes\n",
    "    \n",
    "    return lob_matrix\n",
    "\n",
    "def create_lob_heatmap(lob_matrix, save_path=None):\n",
    "    \"\"\"\n",
    "    Convert the LOB data matrix into a 2D heatmap.\n",
    "    \n",
    "    Args:\n",
    "    - lob_matrix (np.array): 10x4 matrix of normalized bid-ask data.\n",
    "    - save_path (str): Path to save the generated heatmap image.\n",
    "    \n",
    "    Returns:\n",
    "    - None (saves image if save_path is provided).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(lob_matrix, annot=False, cmap=\"coolwarm\", xticklabels=[\"Ask Price\", \"Bid Price\", \"Ask Volume\", \"Bid Volume\"], yticklabels=np.arange(10, 0, -1))\n",
    "    plt.title(\"Limit Order Book Heatmap\")\n",
    "    plt.xlabel(\"Order Type\")\n",
    "    plt.ylabel(\"Price Level\")\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def generate_heatmaps(dataset, save_dir, num_samples=1):\n",
    "    \"\"\"\n",
    "    Generate heatmaps for a given number of LOB snapshots.\n",
    "    \n",
    "    Args:\n",
    "    - dataset (DataFrame): FI-2010 dataset containing LOB snapshots.\n",
    "    - save_dir (str): Directory to save the heatmaps.\n",
    "    - num_samples (int): Number of samples to convert to heatmaps.\n",
    "    \n",
    "    Returns:\n",
    "    - None (Saves images in save_dir).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(f\"Generating {num_samples} heatmaps...\")\n",
    "    for i in tqdm(range(min(num_samples, len(dataset)))):  # Ensuring we don't exceed dataset size\n",
    "        # Extract LOB data\n",
    "        row = dataset.iloc[i].values\n",
    "        \n",
    "        # Split data into bid/ask prices and volumes\n",
    "        bid_prices = row[20:30]  # Columns 20-29 for bid prices\n",
    "        ask_prices = row[0:10]   # Columns 0-9 for ask prices\n",
    "        bid_volumes = row[30:40]  # Columns 30-39 for bid volumes\n",
    "        ask_volumes = row[10:20]  # Columns 10-19 for ask volumes\n",
    "        \n",
    "        # Normalize and convert to 2D matrix\n",
    "        lob_matrix = convert_to_2d_matrix(bid_prices, ask_prices, bid_volumes, ask_volumes)\n",
    "        \n",
    "        # Save heatmap\n",
    "        save_path = os.path.join(save_dir, f\"heatmap_{i}.png\")\n",
    "        create_lob_heatmap(lob_matrix, save_path)\n",
    "\n",
    "# ---- Step 6: Process Labels ----\n",
    "def load_labels(file_path, label_row=144):\n",
    "    \"\"\"\n",
    "    Load the labels (price movement directions).\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the dataset.\n",
    "    - label_row (int): Row where the labels are located (144 for row 145).\n",
    "    \n",
    "    Returns:\n",
    "    - One-hot encoded labels for price movements.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path, delimiter=r'\\s+', header=None)\n",
    "    labels = data.iloc[label_row].values  # Extract labels\n",
    "    one_hot_labels = np.array([one_hot_encode(label) for label in labels])\n",
    "    return one_hot_labels\n",
    "\n",
    "def one_hot_encode(label):\n",
    "    \"\"\"\n",
    "    One-hot encode the label for price movement.\n",
    "    \n",
    "    Args:\n",
    "    - label (int): Price movement label (1, 2, or 3).\n",
    "    \n",
    "    Returns:\n",
    "    - One-hot encoded vector.\n",
    "    \"\"\"\n",
    "    if label == 1:\n",
    "        return [1, 0, 0]\n",
    "    elif label == 2:\n",
    "        return [0, 1, 0]\n",
    "    elif label == 3:\n",
    "        return [0, 0, 1]\n",
    "\n",
    "# ---- Step 7: Final Code to Prepare Data for CNN ----\n",
    "def prepare_data_for_cnn(file_path, num_samples=1, num_levels=10):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for CNN training by processing LOB data and labels.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the FI-2010 dataset.\n",
    "    - num_samples (int): Number of samples to process.\n",
    "    - num_levels (int): Number of price levels.\n",
    "    \n",
    "    Returns:\n",
    "    - X: Feature matrix (LOB snapshots converted to 10x4 matrices).\n",
    "    - y: One-hot encoded labels.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    dataset = load_fi2010_data(file_path, num_levels)\n",
    "    \n",
    "    # Prepare labels\n",
    "    labels = load_labels(file_path)\n",
    "    \n",
    "    # Prepare features (LOB data converted to 10x4 matrices)\n",
    "    X = []\n",
    "    for i in tqdm(range(min(num_samples, len(dataset)))):\n",
    "        row = dataset.iloc[i].values\n",
    "        bid_prices = row[20:30]\n",
    "        ask_prices = row[0:10]\n",
    "        bid_volumes = row[30:40]\n",
    "        ask_volumes = row[10:20]\n",
    "        \n",
    "        lob_matrix = convert_to_2d_matrix(bid_prices, ask_prices, bid_volumes, ask_volumes)\n",
    "        X.append(lob_matrix)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    return X, labels\n",
    "\n",
    "# ---- Example Usage ----\n",
    "file_path = 'Train_DST.txt'  # Replace with your actual file path\n",
    "X, y = prepare_data_for_cnn(file_path, num_samples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
